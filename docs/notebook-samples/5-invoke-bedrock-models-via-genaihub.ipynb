{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup for invoking Anthropic and Titan Family of models via SAP GenAI Hub on SAP AI core\n",
    "\n",
    "## Part 1: Use AI Core REST APIs\n",
    "\n",
    "Use this notebook to invoke the AI Core REST APIs to send your payloads into LLMs hosted on SAP GenAI hub. The documentation for the APIs are provided [here](https://api.sap.com/api/AI_CORE_API/resource/Deployment).\n",
    "\n",
    "\n",
    "## Part 2: Use SAP Generative AI Hub SDK\n",
    "Sample code to invoke Anthropic and Titan Family of models using the ChatBedrock interface in the SAP Generative AI Hub SDK. Documentation [here](https://pypi.org/project/generative-ai-hub-sdk/). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1:\n",
    "### Step 1: Load your AI Core credentials\n",
    "\n",
    "After following the steps in the previous notebook to setup the AI Core instance, Configurations and Deployments for Bedrock models, you can invoke the models via the following approach. \n",
    "\n",
    "First, you should have your AI core creadentials in your ~/.aicore/config.json. If you do not, get it by creating a service key from [here](https://help.sap.com/docs/sap-ai-core/sap-ai-core-service-guide/create-service-key). We will read and consume the data from this location into temporary environment files for this notebook.\n",
    "\n",
    "A sample config.json is below:\n",
    "\n",
    "```sh\n",
    "$ cat ~/.aicore/config.json  \n",
    "{\n",
    "  \"AICORE_AUTH_URL\": \"<>.authentication.us10.hana.ondemand.com\",\n",
    "  \"AICORE_CLIENT_ID\": \"sb-b...64\",\n",
    "  \"AICORE_CLIENT_SECRET\": \"21...xc=\",\n",
    "  \"AICORE_RESOURCE_GROUP\": \"default\",\n",
    "  \"AICORE_BASE_URL\": \"https://api.ai.prod.<>.ondemand.com\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "\n",
    "def load_config():\n",
    "    with open(os.path.expanduser('~/.aicore/config.json')) as f:\n",
    "        config = json.load(f)\n",
    "    for key, value in config.items():\n",
    "        os.environ[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: Uncomment the line which has the model you would like to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://help.sap.com/docs/sap-ai-core/sap-ai-core-service-guide/models-and-scenarios-in-generative-ai-hub or uncomment from below\n",
    "\n",
    "# Multimodal models:\n",
    "os.environ[\"TARGET_AI_CORE_MODEL\"] = \"anthropic--claude-3.5-sonnet\"\n",
    "# os.environ[\"TARGET_AI_CORE_MODEL\"] = \"anthropic--claude-3-opus\"\n",
    "# os.environ[\"TARGET_AI_CORE_MODEL\"] = \"anthropic--claude-3-sonnet\"\n",
    "# os.environ[\"TARGET_AI_CORE_MODEL\"] = \"anthropic--claude-3-haiku\"\n",
    "\n",
    "#Text based models:\n",
    "# os.environ[\"TARGET_AI_CORE_MODEL\"] = \"amazon--titan-text-lite\"\n",
    "# os.environ[\"TARGET_AI_CORE_MODEL\"] = \"amazon--titan-text-express\"\n",
    "\n",
    "# Embedding models:\n",
    "# os.environ[\"TARGET_AI_CORE_MODEL\"] = \"amazon--titan-embed-text\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional Step: If you know the deployment ID from AI Launchpad already, you can uncomment below. This way you don't need to speicify the model name in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL:\n",
    "# Uncomment the below line after pasting your deployment ID if you already know it\n",
    "# os.environ[\"DEPLOYMENT_ID\"] = \"<>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create Utility functions\n",
    "Creating a utility function to send API calls to AI Core instance repeatedly. You will need to install requests library in the terminal \n",
    "```sh\n",
    "pip install requests\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sys\n",
    "import logging\n",
    "import time\n",
    "from util.logging import initLogger\n",
    "\n",
    "TIME_RETRY_API_CALL = 20\n",
    "TIMEOUT_API_CALL = 3600\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "initLogger()\n",
    "\n",
    "\n",
    "# Function to call a rest API\n",
    "def call_api(\n",
    "    type: str, url: str, headers: dict, data: dict = None, message: str = None\n",
    "):\n",
    "    timeNeeded = 0\n",
    "    while timeNeeded < TIMEOUT_API_CALL:\n",
    "        try:\n",
    "            r = None\n",
    "            # Send the request to retrieve the access token\n",
    "            if type == \"POST\":\n",
    "                r = requests.post(url=url, headers=headers, data=data)\n",
    "            elif type == \"GET\":\n",
    "                r = requests.get(url=url, headers=headers)\n",
    "            # if the response is OK, return the JSON response\n",
    "            if r.ok is True:\n",
    "                log.success(f\"{message}\")\n",
    "                return r.json()\n",
    "            else:\n",
    "                log.info(\n",
    "                    f\"response ({message}): {r.status_code} ({r.reason}): {r.text}\"\n",
    "                )\n",
    "                log.warning(\n",
    "                    f\"Could not {message}! Re-trying in {TIME_RETRY_API_CALL} seconds...\"\n",
    "                )\n",
    "                time.sleep(TIME_RETRY_API_CALL)\n",
    "                timeNeeded += TIME_RETRY_API_CALL\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            log.warning(str(e))\n",
    "            log.error(f\"Could not {message}! Exiting...\")\n",
    "            sys.exit(1)\n",
    "    log.error(f\"Could not {message} after {TIMEOUT_API_CALL} seconds! Exiting...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a template to hold the deploymentURL and other imporatant parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from json import JSONEncoder\n",
    "\n",
    "\n",
    "class AiCoreMetadataJsonEncoder(JSONEncoder):\n",
    "    def default(self, o):\n",
    "        return o.__dict__\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AiCoreMetadataDefinition:\n",
    "    authUrl: str\n",
    "    clientId: str\n",
    "    clientSecret: str\n",
    "    apiBase: str\n",
    "    resourceGroup: str\n",
    "    targetAiCoreModel: str\n",
    "    apiAccessToken: str\n",
    "    deploymentUrl : str\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return getattr(self, item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dataClass that will initiate with the environment variables that have the AI Core credentials as well as the target model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AiCoreMetadata(AiCoreMetadataDefinition):\n",
    "    def __init__(self):\n",
    "\n",
    "        load_config()\n",
    "\n",
    "        self.authUrl = os.environ.get(\"AICORE_AUTH_URL\")\n",
    "        self.clientId = os.environ.get(\"AICORE_CLIENT_ID\")\n",
    "        self.clientSecret = os.environ.get(\"AICORE_CLIENT_SECRET\")\n",
    "        self.resourceGroup = os.environ.get(\"AICORE_RESOURCE_GROUP\")\n",
    "        self.apiBase = os.environ.get(\"AICORE_BASE_URL\")\n",
    "        self.targetAiCoreModel = os.environ.get(\"TARGET_AI_CORE_MODEL\")\n",
    "        self.apiAccessToken = get_api_access_token(self)\n",
    "        self.deploymentUrl = get_deployment_details_for_model(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a utility function that will get the API Access Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_api_access_token(aiCoreMetadata: AiCoreMetadataDefinition) -> str:\n",
    "    clientId = aiCoreMetadata.clientId\n",
    "    clientSecret = aiCoreMetadata.clientSecret\n",
    "    authUrl = aiCoreMetadata.authUrl\n",
    "\n",
    "    # Create the authorization string\n",
    "    authorizationString = f\"{clientId}:{clientSecret}\"\n",
    "    # Encode the authorization string\n",
    "    byte_data = authorizationString.encode(\"utf-8\")\n",
    "    # Base64 encode the byte data\n",
    "    clientSecretBase64 = base64.b64encode(byte_data).decode(\"utf-8\")\n",
    "\n",
    "    # Create the URL to retrieve the access token\n",
    "    aiCoreLocation = f\"{authUrl}/oauth/token?grant_type=client_credentials\"\n",
    "    # Create the headers for the request\n",
    "    headers = {\"Authorization\": f\"Basic {clientSecretBase64}\"}\n",
    "\n",
    "    response = call_api(\n",
    "        \"POST\",\n",
    "        aiCoreLocation,\n",
    "        headers,\n",
    "        None,\n",
    "        \"retrieve access token from AI Core system\",\n",
    "    )\n",
    "    # json_response = json.dumps(response, indent=2)\n",
    "\n",
    "    return response[\"access_token\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the API Access token, lets get the deployment URL that corresponds to the Model you want to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the deployment URL from the AI Core system metadata\n",
    "def get_deployment_details_for_model(aiCoreMetadata: AiCoreMetadataDefinition):\n",
    "    apiBase = aiCoreMetadata.apiBase\n",
    "    token = aiCoreMetadata.apiAccessToken\n",
    "    resourceGroup = aiCoreMetadata.resourceGroup\n",
    "\n",
    "    # Create the URL to get the deployment \n",
    "    aiCoreLocation = f\"{apiBase}/v2/lm/deployments\"\n",
    "    # Create the headers for the request\n",
    "    headers = {}\n",
    "    headers[\"AI-Resource-Group\"] = resourceGroup\n",
    "    headers[\"Authorization\"] = f\"Bearer {token}\"\n",
    "    allDeploymentDetails = None\n",
    "\n",
    "    timeNeeded = 0\n",
    "    message = f\"retrieve deployment details for model id {aiCoreMetadata.targetAiCoreModel}\"\n",
    "    while timeNeeded < TIMEOUT_API_CALL:\n",
    "        # Send the request to get the list of deployments\n",
    "        response = call_api(\"GET\", aiCoreLocation, headers, None, message)\n",
    "        json_response = json.dumps(response, indent=2)\n",
    "        # log.check(\n",
    "        #         f\"API response from retrieveing deployment details for model id {aiCoreMetadata.targetAiCoreModel}:\\n{json_response}\"\n",
    "        # )\n",
    "\n",
    "        allDeploymentDetails = response\n",
    "    \n",
    "        for resource in allDeploymentDetails[\"resources\"]:\n",
    "            model_name = resource[\"details\"][\"resources\"][\"backend_details\"][\"model\"][\"name\"]\n",
    "            if model_name == aiCoreMetadata.targetAiCoreModel:\n",
    "                return resource[\"deploymentUrl\"]\n",
    "        \n",
    "        log.warning(\n",
    "                f\"Could not {message} Re-trying in {TIME_RETRY_API_CALL} seconds...\"\n",
    "            )\n",
    "\n",
    "        time.sleep(TIME_RETRY_API_CALL)\n",
    "        timeNeeded += TIME_RETRY_API_CALL\n",
    "\n",
    "    log.error(\n",
    "        f\"Could not retrieve deployment details for id '{aiCoreMetadata.targetAiCoreModel}'! Exiting...\"\n",
    "    )\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another utility function to get deployment details in case deployment ID is provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the deployment URL from the AI Core system metadata\n",
    "def get_deployment_details(aiCoreMetadata: AiCoreMetadataDefinition, deploymenId: str):\n",
    "    apiBase = aiCoreMetadata.apiBase\n",
    "    token = aiCoreMetadata.apiAccessToken\n",
    "    resourceGroup = aiCoreMetadata.resourceGroup\n",
    "\n",
    "    # Create the URL to create the configuration\n",
    "    aiCoreLocation = f\"{apiBase}/v2/lm/deployments/{deploymenId}\"\n",
    "    # Create the headers for the request\n",
    "    headers = {}\n",
    "    headers[\"AI-Resource-Group\"] = resourceGroup\n",
    "    headers[\"Authorization\"] = f\"Bearer {token}\"\n",
    "    deploymentDetails = None\n",
    "\n",
    "    timeNeeded = 0\n",
    "    message = f\"retrieve deployment details for deployment id {deploymenId}\"\n",
    "    while timeNeeded < TIMEOUT_API_CALL:\n",
    "        # Send the request to create the deployment\n",
    "        response = call_api(\"GET\", aiCoreLocation, headers, None, message)\n",
    "        # json_response = json.dumps(response, indent=2)\n",
    "\n",
    "        deploymentDetails = response\n",
    "        deploymentUrl = deploymentDetails[\"deploymentUrl\"]\n",
    "        if deploymentUrl != \"\":\n",
    "            log.success(f\"AI Core deployment id '{deploymenId}' is now accessible!\")\n",
    "            return deploymentDetails\n",
    "        else:\n",
    "            log.warning(\n",
    "                f\"Could not {message} (deployment not finished)! Re-trying in {TIME_RETRY_API_CALL} seconds...\"\n",
    "            )\n",
    "\n",
    "            time.sleep(TIME_RETRY_API_CALL)\n",
    "            timeNeeded += TIME_RETRY_API_CALL\n",
    "\n",
    "    log.error(\n",
    "        f\"Could not retrieve deployment details for id '{deploymenId}'! Exiting...\"\n",
    "    )\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate our class. This step will populate the internal environment variables, execute the access token get utility function and also get the deployment URL for the model specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional Step - execute only if you directly want to specify the deployment URL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you have provided deployment ID directly instead of model id \n",
    "if ai_core_metadata.deploymentUrl == None:\n",
    "    deploymentDetails = get_deployment_details(ai_core_metadata, os.environ[\"DEPLOYMENT_ID\"])\n",
    "    ai_core_metadata.deploymentUrl = deploymentDetails[\"deploymentUrl\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the utility function to send your input prompt to GenAI Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the available AI models from the AI Core system\n",
    "def invoke(aiCoreMetadata: AiCoreMetadataDefinition, \n",
    "           data) -> str:\n",
    "\n",
    "    token = aiCoreMetadata.apiAccessToken\n",
    "    deploymentUrl = aiCoreMetadata.deploymentUrl\n",
    "    \n",
    "    # Create the URL to retrieve the available AI models\n",
    "    aiCoreLocation = f\"{deploymentUrl}/invoke\"\n",
    "    # Create the headers for the request\n",
    "    headers = {\n",
    "        \"AI-Resource-Group\": aiCoreMetadata.resourceGroup,\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "    }\n",
    "    \n",
    "    response = call_api(\n",
    "        \"POST\",\n",
    "        aiCoreLocation,\n",
    "        headers,\n",
    "        json.dumps(data),\n",
    "        \"sending invoke\",\n",
    "    )\n",
    "    json_response = json.dumps(response, indent=2)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 Send the prompt to the GenAI Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define a payload in the messages format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Anthropic Claude models\n",
    "\n",
    "# Multimodal models:\n",
    "os.environ[\"TARGET_AI_CORE_MODEL\"] = \"anthropic--claude-3.5-sonnet\"\n",
    "# os.environ[\"TARGET_AI_CORE_MODEL\"] = \"anthropic--claude-3-opus\"\n",
    "# os.environ[\"TARGET_AI_CORE_MODEL\"] = \"anthropic--claude-3-sonnet\"\n",
    "# os.environ[\"TARGET_AI_CORE_MODEL\"] = \"anthropic--claude-3-haiku\"\n",
    "\n",
    "data = {}\n",
    "messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hello, What is the capital of United States?\"\n",
    "        }\n",
    "    ]\n",
    "data[\"anthropic_version\"] = \"bedrock-2023-05-31\"\n",
    "data[\"max_tokens\"] = 1000\n",
    "data[\"messages\"] = messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;241m[2024-10-02 16:22:17] \u001b[38;5;40mSUCCESS: retrieve access token from AI Core system\u001b[0;0m\n",
      "\u001b[38;5;241m[2024-10-02 16:22:18] \u001b[38;5;40mSUCCESS: retrieve deployment details for model id anthropic--claude-3.5-sonnet\u001b[0;0m\n"
     ]
    }
   ],
   "source": [
    "# Extract the metadata for the AI Core system\n",
    "ai_core_metadata = AiCoreMetadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;241m[2024-10-02 16:22:24] \u001b[38;5;40mSUCCESS: sending invoke\u001b[0;0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'content': [{'text': \"The capital of the United States is Washington, D.C. (District of Columbia). It's located on the east coast of the country, between the states of Maryland and Virginia. Washington, D.C. is the seat of the federal government and home to many important national monuments, museums, and government buildings, including:\\n\\n1. The White House (residence and office of the President)\\n2. The United States Capitol (meeting place of Congress)\\n3. The Supreme Court\\n4. Various federal departments and agencies\\n\\nIt's worth noting that Washington, D.C. is not a state, but a federal district created specifically to serve as the national capital. It was founded in 1790 and named after the first U.S. President, George Washington.\",\n",
       "   'type': 'text'}],\n",
       " 'id': 'msg_bdrk_01UNu67BYNsAzM4tFS7Qw8Jx',\n",
       " 'model': 'claude-3-5-sonnet-20240620',\n",
       " 'role': 'assistant',\n",
       " 'stop_reason': 'end_turn',\n",
       " 'stop_sequence': None,\n",
       " 'type': 'message',\n",
       " 'usage': {'input_tokens': 17, 'output_tokens': 162}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invoke(aiCoreMetadata= ai_core_metadata, \n",
    "       data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Amazon Titan Text Models\n",
    "\n",
    "#Text based models:\n",
    "os.environ[\"TARGET_AI_CORE_MODEL\"] = \"amazon--titan-text-lite\"\n",
    "# os.environ[\"TARGET_AI_CORE_MODEL\"] = \"amazon--titan-text-express\"\n",
    "\n",
    "data = {}\n",
    "\n",
    "data = {\n",
    "    \"inputText\": \"What is the capital of United States?\",\n",
    "    \"textGenerationConfig\": {\n",
    "        \"maxTokenCount\": 1000,\n",
    "        \"stopSequences\": [],\n",
    "        \"temperature\": 0,\n",
    "        \"topP\": 1\n",
    "     }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;241m[2024-10-02 16:22:28] \u001b[38;5;40mSUCCESS: retrieve access token from AI Core system\u001b[0;0m\n",
      "\u001b[38;5;241m[2024-10-02 16:22:28] \u001b[38;5;40mSUCCESS: retrieve deployment details for model id amazon--titan-text-lite\u001b[0;0m\n"
     ]
    }
   ],
   "source": [
    "# Extract the metadata for the AI Core system\n",
    "ai_core_metadata = AiCoreMetadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;241m[2024-10-02 16:22:30] \u001b[38;5;40mSUCCESS: sending invoke\u001b[0;0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'inputTextTokenCount': 8,\n",
       " 'results': [{'completionReason': 'FINISH',\n",
       "   'outputText': '\\nWashington, D.C. is the capital of the United States.',\n",
       "   'tokenCount': 16}]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invoke(aiCoreMetadata= ai_core_metadata, \n",
    "       data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Amazon Titan Embedding Models\n",
    "os.environ[\"TARGET_AI_CORE_MODEL\"] = \"amazon--titan-embed-text\"\n",
    "\n",
    "data = {}\n",
    "\n",
    "data = {\n",
    "    \"inputText\": \"What is the capital of United States?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;241m[2024-10-02 16:22:33] \u001b[38;5;40mSUCCESS: retrieve access token from AI Core system\u001b[0;0m\n",
      "\u001b[38;5;241m[2024-10-02 16:22:33] \u001b[38;5;40mSUCCESS: retrieve deployment details for model id amazon--titan-embed-text\u001b[0;0m\n"
     ]
    }
   ],
   "source": [
    "# Extract the metadata for the AI Core system\n",
    "ai_core_metadata = AiCoreMetadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally get a result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke(aiCoreMetadata= ai_core_metadata, \n",
    "       data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2: Use SAP Generative AI Hub SDK\n",
    "Sample code to invoke Anthropic and Titan Family of models using the ChatBedrock interface in the SAP Generative AI Hub SDK. Documentation [here](https://pypi.org/project/generative-ai-hub-sdk/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install generative-ai-hub-sdk langchain_aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.proxy.langchain.amazon import ChatBedrock\n",
    "from gen_ai_hub.proxy.core.proxy_clients import get_proxy_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_client = get_proxy_client(\"gen-ai-hub\")\n",
    "llm_via_bedrock_on_GenAIHub = ChatBedrock(\n",
    "        model_name=\"anthropic--claude-3.5-sonnet\",\n",
    "        proxy_client=proxy_client,\n",
    "        temperature=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York City offers numerous popular attractions for visitors. Some of the top destinations include:\n",
      "\n",
      "1. Statue of Liberty and Ellis Island\n",
      "2. Empire State Building\n",
      "3. Central Park\n",
      "4. Times Square\n",
      "5. Metropolitan Museum of Art (The Met)\n",
      "6. Broadway Theater District\n",
      "7. Rockefeller Center and Top of the Rock Observation Deck\n",
      "8. 9/11 Memorial and Museum\n",
      "9. High Line\n",
      "10. One World Trade Center and One World Observatory\n",
      "11. Brooklyn Bridge\n",
      "12. Grand Central Terminal\n",
      "13. Museum of Modern Art (MoMA)\n",
      "14. Fifth Avenue (for shopping)\n",
      "15. American Museum of Natural History\n",
      "16. St. Patrick's Cathedral\n",
      "17. Coney Island\n",
      "18. Chinatown and Little Italy\n",
      "19. The Guggenheim Museum\n",
      "20. Hudson Yards and The Vessel\n",
      "\n",
      "These attractions offer a mix of history, culture, art, architecture, and entertainment, making New York City a diverse and exciting destination for visitors.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "        ( \"system\", \"You are a helpful assistant\" ),\n",
    "        ( \"user\", \"Hello, What are the top visitor destinations in NYC?\" )\n",
    "        ]\n",
    "\n",
    "response_body = llm_via_bedrock_on_GenAIHub.invoke(messages)\n",
    "print(response_body.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_via_bedrock_on_GenAIHub_2 = ChatBedrock(\n",
    "        model_name=\"amazon--titan-text-lite\",\n",
    "        proxy_client=proxy_client,\n",
    "        temperature=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello, The capital of the United States is Washington, D.C.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "        ( \"system\", \"You are a helpful assistant\" ),\n",
    "        ( \"user\", \"Hello, What is the capital of United States?\" )\n",
    "        ]\n",
    "\n",
    "response_body = llm_via_bedrock_on_GenAIHub_2.invoke(messages)\n",
    "print(response_body.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
