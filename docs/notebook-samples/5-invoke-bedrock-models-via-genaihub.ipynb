{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup for invoking Anthropic and Titan Family of models via SAP GenAI Hub on SAP AI core\n",
    "\n",
    "## Part 1: Use AI Core REST APIs\n",
    "\n",
    "Use this notebook to invoke the AI Core REST APIs to send your payloads into LLMs hosted on SAP GenAI hub. The documentation for the APIs are provided [here](https://api.sap.com/api/AI_CORE_API/resource/Deployment).\n",
    "\n",
    "\n",
    "## Part 2: Use SAP Generative AI Hub SDK\n",
    "Sample code to invoke Anthropic and Titan Family of models using the ChatBedrock interface in the SAP Generative AI Hub SDK. Documentation [here](https://pypi.org/project/generative-ai-hub-sdk/). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1:\n",
    "### Load your AI Core credentials\n",
    "\n",
    "After following the steps in the previous notebook to setup the AI Core instance, Configurations and Deployments for Bedrock models, you can invoke the models via the following approach. \n",
    "\n",
    "First, you should have your AI core creadentials in your ~/.aicore/config.json. If you do not, get it by creating a service key from [here](https://help.sap.com/docs/sap-ai-core/sap-ai-core-service-guide/create-service-key). We will read and consume the data from this location into temporary environment files for this notebook.\n",
    "\n",
    "A sample config.json is below:\n",
    "\n",
    "```sh\n",
    "$ cat ~/.aicore/config.json  \n",
    "{\n",
    "  \"AICORE_AUTH_URL\": \"<>.authentication.us10.hana.ondemand.com\",\n",
    "  \"AICORE_CLIENT_ID\": \"sb-b...64\",\n",
    "  \"AICORE_CLIENT_SECRET\": \"21...xc=\",\n",
    "  \"AICORE_RESOURCE_GROUP\": \"default\",\n",
    "  \"AICORE_BASE_URL\": \"https://api.ai.prod.<>.ondemand.com\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "\n",
    "def load_config():\n",
    "    with open(os.path.expanduser('~/.aicore/config.json')) as f:\n",
    "        config = json.load(f)\n",
    "    for key, value in config.items():\n",
    "        os.environ[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://help.sap.com/docs/sap-ai-core/sap-ai-core-service-guide/models-and-scenarios-in-generative-ai-hub or uncomment from below\n",
    "\n",
    "# Multimodal models:\n",
    "os.environ[\"TARGET_AI_CORE_MODEL\"] = \"anthropic--claude-3.5-sonnet\"\n",
    "# os.environ[\"TARGET_AI_CORE_MODEL\"] = \"anthropic--claude-3-opus\"\n",
    "# os.environ[\"TARGET_AI_CORE_MODEL\"] = \"anthropic--claude-3-sonnet\"\n",
    "# os.environ[\"TARGET_AI_CORE_MODEL\"] = \"anthropic--claude-3-haiku\"\n",
    "\n",
    "#Text based models:\n",
    "# os.environ[\"TARGET_AI_CORE_MODEL\"] = \"amazon--titan-text-lite\"\n",
    "# os.environ[\"TARGET_AI_CORE_MODEL\"] = \"amazon--titan-text-express\"\n",
    "\n",
    "# Embedding models:\n",
    "# os.environ[\"TARGET_AI_CORE_MODEL\"] = \"amazon--titan-embed-text\"\n",
    "\n",
    "# Uncomment the below line after pasting your deployment ID if you already know it\n",
    "# os.environ[\"DEPLOYMENT_ID\"] = \"<>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a utility function to send API calls to AI Core instance repeatedly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sys\n",
    "import logging\n",
    "import time\n",
    "from util.logging import initLogger\n",
    "\n",
    "TIME_RETRY_API_CALL = 20\n",
    "TIMEOUT_API_CALL = 3600\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "initLogger()\n",
    "\n",
    "\n",
    "# Function to call a rest API\n",
    "def call_api(\n",
    "    type: str, url: str, headers: dict, data: dict = None, message: str = None\n",
    "):\n",
    "    timeNeeded = 0\n",
    "    while timeNeeded < TIMEOUT_API_CALL:\n",
    "        try:\n",
    "            r = None\n",
    "            # Send the request to retrieve the access token\n",
    "            if type == \"POST\":\n",
    "                r = requests.post(url=url, headers=headers, data=data)\n",
    "            elif type == \"GET\":\n",
    "                r = requests.get(url=url, headers=headers)\n",
    "            # if the response is OK, return the JSON response\n",
    "            if r.ok is True:\n",
    "                log.success(f\"{message}\")\n",
    "                return r.json()\n",
    "            else:\n",
    "                log.info(\n",
    "                    f\"response ({message}): {r.status_code} ({r.reason}): {r.text}\"\n",
    "                )\n",
    "                log.warning(\n",
    "                    f\"Could not {message}! Re-trying in {TIME_RETRY_API_CALL} seconds...\"\n",
    "                )\n",
    "                time.sleep(TIME_RETRY_API_CALL)\n",
    "                timeNeeded += TIME_RETRY_API_CALL\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            log.warning(str(e))\n",
    "            log.error(f\"Could not {message}! Exiting...\")\n",
    "            sys.exit(1)\n",
    "    log.error(f\"Could not {message} after {TIMEOUT_API_CALL} seconds! Exiting...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from json import JSONEncoder\n",
    "\n",
    "\n",
    "class AiCoreMetadataJsonEncoder(JSONEncoder):\n",
    "    def default(self, o):\n",
    "        return o.__dict__\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AiCoreMetadataDefinition:\n",
    "    authUrl: str\n",
    "    clientId: str\n",
    "    clientSecret: str\n",
    "    apiBase: str\n",
    "    resourceGroup: str\n",
    "    targetAiCoreModel: str\n",
    "    apiAccessToken: str\n",
    "    deploymentUrl : str\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return getattr(self, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AiCoreMetadata(AiCoreMetadataDefinition):\n",
    "    def __init__(self):\n",
    "\n",
    "        load_config()\n",
    "\n",
    "        self.authUrl = os.environ.get(\"AICORE_AUTH_URL\")\n",
    "        self.clientId = os.environ.get(\"AICORE_CLIENT_ID\")\n",
    "        self.clientSecret = os.environ.get(\"AICORE_CLIENT_SECRET\")\n",
    "        self.resourceGroup = os.environ.get(\"AICORE_RESOURCE_GROUP\")\n",
    "        self.apiBase = os.environ.get(\"AICORE_BASE_URL\")\n",
    "        self.targetAiCoreModel = os.environ.get(\"TARGET_AI_CORE_MODEL\")\n",
    "        self.apiAccessToken = get_api_access_token(self)\n",
    "        self.deploymentUrl = get_deployment_details_for_model(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_api_access_token(aiCoreMetadata: AiCoreMetadataDefinition) -> str:\n",
    "    clientId = aiCoreMetadata.clientId\n",
    "    clientSecret = aiCoreMetadata.clientSecret\n",
    "    authUrl = aiCoreMetadata.authUrl\n",
    "\n",
    "    # Create the authorization string\n",
    "    authorizationString = f\"{clientId}:{clientSecret}\"\n",
    "    # Encode the authorization string\n",
    "    byte_data = authorizationString.encode(\"utf-8\")\n",
    "    # Base64 encode the byte data\n",
    "    clientSecretBase64 = base64.b64encode(byte_data).decode(\"utf-8\")\n",
    "\n",
    "    # Create the URL to retrieve the access token\n",
    "    aiCoreLocation = f\"{authUrl}/oauth/token?grant_type=client_credentials\"\n",
    "    # Create the headers for the request\n",
    "    headers = {\"Authorization\": f\"Basic {clientSecretBase64}\"}\n",
    "\n",
    "    response = call_api(\n",
    "        \"POST\",\n",
    "        aiCoreLocation,\n",
    "        headers,\n",
    "        None,\n",
    "        \"retrieve access token from AI Core system\",\n",
    "    )\n",
    "    # json_response = json.dumps(response, indent=2)\n",
    "\n",
    "    return response[\"access_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the deployment URL from the AI Core system metadata\n",
    "def get_deployment_details_for_model(aiCoreMetadata: AiCoreMetadataDefinition):\n",
    "    apiBase = aiCoreMetadata.apiBase\n",
    "    token = aiCoreMetadata.apiAccessToken\n",
    "    resourceGroup = aiCoreMetadata.resourceGroup\n",
    "\n",
    "    # Create the URL to get the deployment \n",
    "    aiCoreLocation = f\"{apiBase}/v2/lm/deployments\"\n",
    "    # Create the headers for the request\n",
    "    headers = {}\n",
    "    headers[\"AI-Resource-Group\"] = resourceGroup\n",
    "    headers[\"Authorization\"] = f\"Bearer {token}\"\n",
    "    allDeploymentDetails = None\n",
    "\n",
    "    timeNeeded = 0\n",
    "    message = f\"retrieve deployment details for model id {aiCoreMetadata.targetAiCoreModel}\"\n",
    "    while timeNeeded < TIMEOUT_API_CALL:\n",
    "        # Send the request to get the list of deployments\n",
    "        response = call_api(\"GET\", aiCoreLocation, headers, None, message)\n",
    "        json_response = json.dumps(response, indent=2)\n",
    "        log.check(\n",
    "                f\"API response from retrieveing deployment details for model id {aiCoreMetadata.targetAiCoreModel}:\\n{json_response}\"\n",
    "        )\n",
    "\n",
    "        allDeploymentDetails = response\n",
    "    \n",
    "        for resource in allDeploymentDetails[\"resources\"]:\n",
    "            model_name = resource[\"details\"][\"resources\"][\"backend_details\"][\"model\"][\"name\"]\n",
    "            if model_name == aiCoreMetadata.targetAiCoreModel:\n",
    "                return resource[\"deploymentUrl\"]\n",
    "        \n",
    "        log.warning(\n",
    "                f\"Could not {message} Re-trying in {TIME_RETRY_API_CALL} seconds...\"\n",
    "            )\n",
    "\n",
    "        time.sleep(TIME_RETRY_API_CALL)\n",
    "        timeNeeded += TIME_RETRY_API_CALL\n",
    "\n",
    "    log.error(\n",
    "        f\"Could not retrieve deployment details for id '{aiCoreMetadata.targetAiCoreModel}'! Exiting...\"\n",
    "    )\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the deployment URL from the AI Core system metadata\n",
    "def get_deployment_details(aiCoreMetadata: AiCoreMetadataDefinition, deploymenId: str):\n",
    "    apiBase = aiCoreMetadata.apiBase\n",
    "    token = aiCoreMetadata.apiAccessToken\n",
    "    resourceGroup = aiCoreMetadata.resourceGroup\n",
    "\n",
    "    # Create the URL to create the configuration\n",
    "    aiCoreLocation = f\"{apiBase}/v2/lm/deployments/{deploymenId}\"\n",
    "    # Create the headers for the request\n",
    "    headers = {}\n",
    "    headers[\"AI-Resource-Group\"] = resourceGroup\n",
    "    headers[\"Authorization\"] = f\"Bearer {token}\"\n",
    "    deploymentDetails = None\n",
    "\n",
    "    timeNeeded = 0\n",
    "    message = f\"retrieve deployment details for deployment id {deploymenId}\"\n",
    "    while timeNeeded < TIMEOUT_API_CALL:\n",
    "        # Send the request to create the deployment\n",
    "        response = call_api(\"GET\", aiCoreLocation, headers, None, message)\n",
    "        # json_response = json.dumps(response, indent=2)\n",
    "\n",
    "        deploymentDetails = response\n",
    "        deploymentUrl = deploymentDetails[\"deploymentUrl\"]\n",
    "        if deploymentUrl != \"\":\n",
    "            log.success(f\"AI Core deployment id '{deploymenId}' is now accessible!\")\n",
    "            return deploymentDetails\n",
    "        else:\n",
    "            log.warning(\n",
    "                f\"Could not {message} (deployment not finished)! Re-trying in {TIME_RETRY_API_CALL} seconds...\"\n",
    "            )\n",
    "\n",
    "            time.sleep(TIME_RETRY_API_CALL)\n",
    "            timeNeeded += TIME_RETRY_API_CALL\n",
    "\n",
    "    log.error(\n",
    "        f\"Could not retrieve deployment details for id '{deploymenId}'! Exiting...\"\n",
    "    )\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the metadata for the AI Core system\n",
    "ai_core_metadata = AiCoreMetadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you have provided deployment ID directly instead of model id \n",
    "if ai_core_metadata.deploymentUrl == None:\n",
    "    deploymentDetails = get_deployment_details(ai_core_metadata, os.environ[\"DEPLOYMENT_ID\"])\n",
    "    ai_core_metadata.deploymentUrl = deploymentDetails[\"deploymentUrl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the available AI models from the AI Core system\n",
    "def invoke(aiCoreMetadata: AiCoreMetadataDefinition, \n",
    "           messages,\n",
    "           max_tokens = 1000) -> str:\n",
    "\n",
    "    token = aiCoreMetadata.apiAccessToken\n",
    "    deploymentUrl = aiCoreMetadata.deploymentUrl\n",
    "    \n",
    "    # Create the URL to retrieve the available AI models\n",
    "    aiCoreLocation = f\"{deploymentUrl}/invoke\"\n",
    "    # Create the headers for the request\n",
    "    headers = {\n",
    "        \"AI-Resource-Group\": aiCoreMetadata.resourceGroup,\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "    }\n",
    "\n",
    "    data = {}\n",
    "    data[\"anthropic_version\"] = \"bedrock-2023-05-31\"\n",
    "    data[\"max_tokens\"] = max_tokens\n",
    "    data[\"messages\"] = messages\n",
    "    \n",
    "    response = call_api(\n",
    "        \"POST\",\n",
    "        aiCoreLocation,\n",
    "        headers,\n",
    "        json.dumps(data),\n",
    "        \"sending invoke\",\n",
    "    )\n",
    "    json_response = json.dumps(response, indent=2)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hello, Claude\"\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke(aiCoreMetadata= ai_core_metadata, \n",
    "       messages=messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2: Use SAP Generative AI Hub SDK\n",
    "Sample code to invoke Anthropic and Titan Family of models using the ChatBedrock interface in the SAP Generative AI Hub SDK. Documentation [here](https://pypi.org/project/generative-ai-hub-sdk/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip -q install generative-ai-hub-sdk==1.11.1 langchain_aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.proxy.langchain.amazon import ChatBedrock\n",
    "from gen_ai_hub.proxy.core.proxy_clients import get_proxy_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_client = get_proxy_client(\"gen-ai-hub\")\n",
    "llm_via_bedrock_on_GenAIHub = ChatBedrock(\n",
    "        model_name=\"anthropic--claude-3-sonnet\",\n",
    "        proxy_client=proxy_client,\n",
    "        temperature=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "        ( \"system\", \"You are a helpful assistant\" ),\n",
    "        ( \"user\", \"Hello, Claude\" )\n",
    "        ]\n",
    "\n",
    "response_body = llm_via_bedrock_on_GenAIHub.invoke(messages)\n",
    "print(response_body.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
